{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "PATH = \"/content/Hindi_English_Truncated_Corpus.csv\"\n"
      ],
      "metadata": {
        "id": "8moFAjj5u6R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    if isinstance(w, np.ndarray):\n",
        "        w = ' '.join(w)\n",
        "\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "evvrjlAXydRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    lines = lines[lines['source']=='ted']\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ],
      "metadata": {
        "id": "mwwjps21ygOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "metadata": {
        "id": "aNRrmDH-yowC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "BIGeBoNQyqY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "gRM3qsbPys5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "metadata": {
        "id": "a-HA4-g6yug9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ofQlJ5L1Xmj",
        "outputId": "dbf0c935-d353-448e-abe2-bd6d4c228e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31904 31904 7977 7977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-S3xrzK1m-J",
        "outputId": "ab21ed68-4f49-43ce-9dd3-655e25f53da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> and\n",
            "72 ----> then\n",
            "159 ----> say\n",
            "5 ----> to\n",
            "986 ----> yourself\n",
            "15247 ----> quietly ,\n",
            "23 ----> but\n",
            "25 ----> with\n",
            "9725 ----> determination\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4311 ----> [और\n",
            "234 ----> फ़िर\n",
            "210 ----> खुद\n",
            "7 ----> से\n",
            "2576 ----> चुपचाप\n",
            "3700 ----> कहिये,\n",
            "134 ----> मगर\n",
            "2137 ----> ठोस\n",
            "9630 ----> भरोसे\n",
            "4 ----> के\n",
            "15475 ----> साथ:]\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "tdXJ8u0a13CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(self.enc_units,\n",
        "                                             return_sequences=True,\n",
        "                                             return_state=True,\n",
        "                                             recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.rnn(x, initial_state=hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "waI89-Oe14-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "W6nnLyhu18JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(self.dec_units,\n",
        "                                             return_sequences=True,\n",
        "                                             return_state=True,\n",
        "                                             recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.rnn(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "nHoQVDag2HEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "BZBcIrr82LW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "4afpjDEV2NsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "6K7SX2f_2PXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLfNjpfw2Q_5",
        "outputId": "eaab2d16-c2cf-4821-c014-7268ad2fcb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.8066\n",
            "Epoch 1 Batch 100 Loss 1.8385\n",
            "Epoch 1 Batch 200 Loss 1.8314\n",
            "Epoch 1 Batch 300 Loss 1.8051\n",
            "Epoch 1 Batch 400 Loss 1.8776\n",
            "Epoch 1 Loss 1.9881\n",
            "Time taken for 1 epoch 94.99504852294922 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6184\n",
            "Epoch 2 Batch 100 Loss 1.8992\n",
            "Epoch 2 Batch 200 Loss 1.7488\n",
            "Epoch 2 Batch 300 Loss 1.8070\n",
            "Epoch 2 Batch 400 Loss 1.6260\n",
            "Epoch 2 Loss 1.7103\n",
            "Time taken for 1 epoch 50.214624881744385 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6211\n",
            "Epoch 3 Batch 100 Loss 1.5206\n",
            "Epoch 3 Batch 200 Loss 1.6373\n",
            "Epoch 3 Batch 300 Loss 1.4837\n",
            "Epoch 3 Batch 400 Loss 1.6594\n",
            "Epoch 3 Loss 1.5515\n",
            "Time taken for 1 epoch 49.83439779281616 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4177\n",
            "Epoch 4 Batch 100 Loss 1.3050\n",
            "Epoch 4 Batch 200 Loss 1.4193\n",
            "Epoch 4 Batch 300 Loss 1.3493\n",
            "Epoch 4 Batch 400 Loss 1.4869\n",
            "Epoch 4 Loss 1.4378\n",
            "Time taken for 1 epoch 49.941410064697266 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3905\n",
            "Epoch 5 Batch 100 Loss 1.3585\n",
            "Epoch 5 Batch 200 Loss 1.3640\n",
            "Epoch 5 Batch 300 Loss 1.3199\n",
            "Epoch 5 Batch 400 Loss 1.2869\n",
            "Epoch 5 Loss 1.3384\n",
            "Time taken for 1 epoch 49.5923810005188 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3526\n",
            "Epoch 6 Batch 100 Loss 1.2472\n",
            "Epoch 6 Batch 200 Loss 1.3752\n",
            "Epoch 6 Batch 300 Loss 1.2449\n",
            "Epoch 6 Batch 400 Loss 1.2987\n",
            "Epoch 6 Loss 1.2481\n",
            "Time taken for 1 epoch 51.61242055892944 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0737\n",
            "Epoch 7 Batch 100 Loss 1.2194\n",
            "Epoch 7 Batch 200 Loss 1.1585\n",
            "Epoch 7 Batch 300 Loss 1.3050\n",
            "Epoch 7 Batch 400 Loss 1.1571\n",
            "Epoch 7 Loss 1.1636\n",
            "Time taken for 1 epoch 48.525773763656616 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1196\n",
            "Epoch 8 Batch 100 Loss 1.1224\n",
            "Epoch 8 Batch 200 Loss 1.0631\n",
            "Epoch 8 Batch 300 Loss 1.1712\n",
            "Epoch 8 Batch 400 Loss 1.0488\n",
            "Epoch 8 Loss 1.0846\n",
            "Time taken for 1 epoch 49.34065532684326 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0273\n",
            "Epoch 9 Batch 100 Loss 0.9982\n",
            "Epoch 9 Batch 200 Loss 0.9486\n",
            "Epoch 9 Batch 300 Loss 0.9771\n",
            "Epoch 9 Batch 400 Loss 1.0201\n",
            "Epoch 9 Loss 1.0119\n",
            "Time taken for 1 epoch 48.94623899459839 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0027\n",
            "Epoch 10 Batch 100 Loss 0.9046\n",
            "Epoch 10 Batch 200 Loss 0.8894\n",
            "Epoch 10 Batch 300 Loss 0.9892\n",
            "Epoch 10 Batch 400 Loss 1.0921\n",
            "Epoch 10 Loss 0.9431\n",
            "Time taken for 1 epoch 49.94662880897522 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8323\n",
            "Epoch 11 Batch 100 Loss 0.7738\n",
            "Epoch 11 Batch 200 Loss 0.7681\n",
            "Epoch 11 Batch 300 Loss 0.9026\n",
            "Epoch 11 Batch 400 Loss 0.8992\n",
            "Epoch 11 Loss 0.8820\n",
            "Time taken for 1 epoch 48.91502404212952 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.7896\n",
            "Epoch 12 Batch 100 Loss 0.7944\n",
            "Epoch 12 Batch 200 Loss 0.7482\n",
            "Epoch 12 Batch 300 Loss 0.8204\n",
            "Epoch 12 Batch 400 Loss 0.8157\n",
            "Epoch 12 Loss 0.8263\n",
            "Time taken for 1 epoch 48.75108790397644 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7519\n",
            "Epoch 13 Batch 100 Loss 0.7108\n",
            "Epoch 13 Batch 200 Loss 0.8658\n",
            "Epoch 13 Batch 300 Loss 0.7611\n",
            "Epoch 13 Batch 400 Loss 0.6222\n",
            "Epoch 13 Loss 0.7761\n",
            "Time taken for 1 epoch 48.299328327178955 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6816\n",
            "Epoch 14 Batch 100 Loss 0.7022\n",
            "Epoch 14 Batch 200 Loss 0.6943\n",
            "Epoch 14 Batch 300 Loss 0.7181\n",
            "Epoch 14 Batch 400 Loss 0.7532\n",
            "Epoch 14 Loss 0.7296\n",
            "Time taken for 1 epoch 49.52467322349548 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.6953\n",
            "Epoch 15 Batch 100 Loss 0.6292\n",
            "Epoch 15 Batch 200 Loss 0.6028\n",
            "Epoch 15 Batch 300 Loss 0.7571\n",
            "Epoch 15 Batch 400 Loss 0.6685\n",
            "Epoch 15 Loss 0.6877\n",
            "Time taken for 1 epoch 49.66863441467285 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.6291\n",
            "Epoch 16 Batch 100 Loss 0.6521\n",
            "Epoch 16 Batch 200 Loss 0.5111\n",
            "Epoch 16 Batch 300 Loss 0.7243\n",
            "Epoch 16 Batch 400 Loss 0.6772\n",
            "Epoch 16 Loss 0.6502\n",
            "Time taken for 1 epoch 49.154369831085205 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.6041\n",
            "Epoch 17 Batch 100 Loss 0.5684\n",
            "Epoch 17 Batch 200 Loss 0.6014\n",
            "Epoch 17 Batch 300 Loss 0.5944\n",
            "Epoch 17 Batch 400 Loss 0.5548\n",
            "Epoch 17 Loss 0.6155\n",
            "Time taken for 1 epoch 48.14159846305847 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.5295\n",
            "Epoch 18 Batch 100 Loss 0.6508\n",
            "Epoch 18 Batch 200 Loss 0.6199\n",
            "Epoch 18 Batch 300 Loss 0.5872\n",
            "Epoch 18 Batch 400 Loss 0.6117\n",
            "Epoch 18 Loss 0.5815\n",
            "Time taken for 1 epoch 48.47032880783081 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.3950\n",
            "Epoch 19 Batch 100 Loss 0.5613\n",
            "Epoch 19 Batch 200 Loss 0.6060\n",
            "Epoch 19 Batch 300 Loss 0.5608\n",
            "Epoch 19 Batch 400 Loss 0.6648\n",
            "Epoch 19 Loss 0.5511\n",
            "Time taken for 1 epoch 48.69607949256897 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.5102\n",
            "Epoch 20 Batch 100 Loss 0.5656\n",
            "Epoch 20 Batch 200 Loss 0.4522\n",
            "Epoch 20 Batch 300 Loss 0.5029\n",
            "Epoch 20 Batch 400 Loss 0.5143\n",
            "Epoch 20 Loss 0.5247\n",
            "Time taken for 1 epoch 49.119240045547485 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence\n"
      ],
      "metadata": {
        "id": "pMkabD6y6gEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n"
      ],
      "metadata": {
        "id": "B-M304hd6hyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czP5yg9K6jKD",
        "outputId": "e9aca1cd-e35b-44a9-c77d-e80d2f0ad18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x786897677e50>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_input = input(\"Enter a sentence in English: \")\n",
        "translate(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F1LGKjN6kTK",
        "outputId": "77dd6137-5007-4ffd-d593-ebb60649437f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence in English: did you go home\n",
            "Input: did you go home\n",
            "Predicted translation: मैंने उन्हें वापस जाइये जाओ अपने आप को घर वापस वापस वापस जाओ घर की आप वापस वापस वापस वापस वापस वापस जाओ घर की आप वापस वापस वापस वापस वापस वापस जाओ \n"
          ]
        }
      ]
    }
  ]
}